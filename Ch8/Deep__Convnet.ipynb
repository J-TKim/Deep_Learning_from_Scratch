{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.288254923651527\n",
      "=== epoch:1, train acc:0.108, test acc:0.12 ===\n",
      "train loss:2.255065205782584\n",
      "train loss:2.2826273418113043\n",
      "train loss:2.240577747805224\n",
      "train loss:2.2845706634495317\n",
      "train loss:2.2966264995233754\n",
      "train loss:2.245526680016026\n",
      "train loss:2.2390687512392007\n",
      "train loss:2.226368457790446\n",
      "train loss:2.2240758115946226\n",
      "train loss:2.196161640482125\n",
      "train loss:2.177881983479468\n",
      "train loss:2.123435490534197\n",
      "train loss:2.0873173906028186\n",
      "train loss:2.127666740185726\n",
      "train loss:2.2044373982147722\n",
      "train loss:2.04158508784568\n",
      "train loss:2.0132872263154598\n",
      "train loss:2.049279718288356\n",
      "train loss:2.1101028100441312\n",
      "train loss:1.9691273833395653\n",
      "train loss:2.0592533475276884\n",
      "train loss:1.9869641311995951\n",
      "train loss:1.918995072932669\n",
      "train loss:1.9456813462821538\n",
      "train loss:2.006396576666892\n",
      "train loss:1.9830486514723489\n",
      "train loss:1.9751421528650936\n",
      "train loss:1.9360351037647479\n",
      "train loss:2.0326519742971456\n",
      "train loss:1.8657244942672526\n",
      "train loss:1.8081088696261358\n",
      "train loss:1.969541903796856\n",
      "train loss:1.9593364369349513\n",
      "train loss:1.6980024234549995\n",
      "train loss:1.8684263890791555\n",
      "train loss:1.7765930495646978\n",
      "train loss:1.8180569395022022\n",
      "train loss:1.8074956583358648\n",
      "train loss:1.6576764200565788\n",
      "train loss:1.701270747118187\n",
      "train loss:1.8112078950014208\n",
      "train loss:1.7304853009531158\n",
      "train loss:1.6806497931205338\n",
      "train loss:1.9648288350561154\n",
      "train loss:1.8687416913254091\n",
      "train loss:1.838234509332403\n",
      "train loss:1.7229688535908403\n",
      "train loss:1.6966829347672177\n",
      "train loss:1.6411555095371981\n",
      "train loss:1.7192862899495913\n",
      "=== epoch:2, train acc:0.838, test acc:0.796 ===\n",
      "train loss:1.7633533596289805\n",
      "train loss:1.5163978389081794\n",
      "train loss:1.7326817998351656\n",
      "train loss:1.6466099517585624\n",
      "train loss:1.5553616874565932\n",
      "train loss:1.861567925156982\n",
      "train loss:1.652803603874559\n",
      "train loss:1.5331021871162565\n",
      "train loss:1.694525622249273\n",
      "train loss:1.61636215450024\n",
      "train loss:1.5392108128490707\n",
      "train loss:1.6440319420949168\n",
      "train loss:1.6215440289274892\n",
      "train loss:1.6356474075338463\n",
      "train loss:1.6881426479238275\n",
      "train loss:1.695914415471078\n",
      "train loss:1.641653859472388\n",
      "train loss:1.6540180092537853\n",
      "train loss:1.603103010616483\n",
      "train loss:1.6186703698654696\n",
      "train loss:1.6131443677908737\n",
      "train loss:1.6958426377869156\n",
      "train loss:1.4108788492159283\n",
      "train loss:1.4722585558617303\n",
      "train loss:1.6749231886697702\n",
      "train loss:1.7768556367855597\n",
      "train loss:1.32750712316798\n",
      "train loss:1.604474980364077\n",
      "train loss:1.6002743358186737\n",
      "train loss:1.5809809622593667\n",
      "train loss:1.4442800278416001\n",
      "train loss:1.4755978646755679\n",
      "train loss:1.7154672627344671\n",
      "train loss:1.3572550580948388\n",
      "train loss:1.7074383525750698\n",
      "train loss:1.5776599651513163\n",
      "train loss:1.5574390952248536\n",
      "train loss:1.573572536833553\n",
      "train loss:1.5720500910717894\n",
      "train loss:1.7701386055529156\n",
      "train loss:1.5693494178270193\n",
      "train loss:1.3311278822953643\n",
      "train loss:1.5592752314811655\n",
      "train loss:1.5764331489616266\n",
      "train loss:1.5480671142122484\n",
      "train loss:1.474061484077675\n",
      "train loss:1.5352303985806959\n",
      "train loss:1.6306461459842299\n",
      "train loss:1.5726635993116336\n",
      "train loss:1.4472507015278075\n",
      "=== epoch:3, train acc:0.924, test acc:0.921 ===\n",
      "train loss:1.5599284321757394\n",
      "train loss:1.619050470243279\n",
      "train loss:1.3828614784030722\n",
      "train loss:1.5584589152996782\n",
      "train loss:1.4237893666029313\n",
      "train loss:1.7150264031899278\n",
      "train loss:1.4448901684850182\n",
      "train loss:1.5902101130543524\n",
      "train loss:1.5737648589298985\n",
      "train loss:1.3845792593630177\n",
      "train loss:1.3638231325410777\n",
      "train loss:1.5212364208637736\n",
      "train loss:1.4455160654791959\n",
      "train loss:1.6125941876706036\n",
      "train loss:1.4777440922432816\n",
      "train loss:1.3633405322304717\n",
      "train loss:1.4824968749029341\n",
      "train loss:1.2213425960577597\n",
      "train loss:1.3587556870162651\n",
      "train loss:1.306465866925035\n",
      "train loss:1.2772678156504547\n",
      "train loss:1.437611727750911\n",
      "train loss:1.408170549717313\n",
      "train loss:1.351609024568424\n",
      "train loss:1.4925960667611062\n",
      "train loss:1.2772235699968824\n",
      "train loss:1.5912682473362079\n",
      "train loss:1.4254085169791113\n",
      "train loss:1.4492810026966345\n",
      "train loss:1.5114585510849343\n",
      "train loss:1.3164436999365878\n",
      "train loss:1.2785818641222462\n",
      "train loss:1.362207790593342\n",
      "train loss:1.4633627870795136\n",
      "train loss:1.5057589856401747\n",
      "train loss:1.244377761384669\n",
      "train loss:1.4739538687745202\n",
      "train loss:1.431087359419312\n",
      "train loss:1.25283857639664\n",
      "train loss:1.351550976259644\n",
      "train loss:1.3421167642828034\n",
      "train loss:1.3596142646843266\n",
      "train loss:1.4081131764360988\n",
      "train loss:1.2345846570463153\n",
      "train loss:1.3088894866097933\n",
      "train loss:1.380215989553477\n",
      "train loss:1.3977880316519664\n",
      "train loss:1.2558755187999797\n",
      "train loss:1.4378141914343683\n",
      "train loss:1.3820182012740987\n",
      "=== epoch:4, train acc:0.946, test acc:0.918 ===\n",
      "train loss:1.2492315077635638\n",
      "train loss:1.3137722775518605\n",
      "train loss:1.2878980455473517\n",
      "train loss:1.4090051257043887\n",
      "train loss:1.369848554608888\n",
      "train loss:1.3242886242989227\n",
      "train loss:1.3146445067212043\n",
      "train loss:1.21449260241353\n",
      "train loss:1.2030089759489564\n",
      "train loss:1.293399242623523\n",
      "train loss:1.3339897503128695\n",
      "train loss:1.4071178972965583\n",
      "train loss:1.3229300958742456\n",
      "train loss:1.238990318613105\n",
      "train loss:1.2607992668026302\n",
      "train loss:1.2326506373977637\n",
      "train loss:1.2461442860083007\n",
      "train loss:1.2856402016567738\n",
      "train loss:1.1421191956822387\n",
      "train loss:1.3443497411551144\n",
      "train loss:1.2676252685751517\n",
      "train loss:1.2846220590622204\n",
      "train loss:1.2725122391569972\n",
      "train loss:1.382866839639359\n",
      "train loss:1.1637708718510733\n",
      "train loss:1.3154824948789632\n",
      "train loss:1.2527644296236438\n",
      "train loss:1.1529599107102202\n",
      "train loss:1.2770614258873598\n",
      "train loss:1.141953082576731\n",
      "train loss:1.2711893455421883\n",
      "train loss:1.442152350164492\n",
      "train loss:1.446508370125461\n",
      "train loss:1.4809142241470825\n",
      "train loss:1.4467858549421677\n",
      "train loss:1.4882638792292817\n",
      "train loss:1.3163676585334927\n",
      "train loss:1.278972760176218\n",
      "train loss:1.3124958435269969\n",
      "train loss:1.0574940938894692\n",
      "train loss:1.2634856765738542\n",
      "train loss:1.3681494261285534\n",
      "train loss:1.395373510605809\n",
      "train loss:1.3470611917356863\n",
      "train loss:1.3211958041720382\n",
      "train loss:1.2874607075936957\n",
      "train loss:1.1788633917410964\n",
      "train loss:1.429250494829529\n",
      "train loss:1.1832075122503622\n",
      "train loss:1.290635289832405\n",
      "=== epoch:5, train acc:0.951, test acc:0.925 ===\n",
      "train loss:1.1127542056911728\n",
      "train loss:1.4707815357701162\n",
      "train loss:1.308023112341624\n",
      "train loss:1.3792546776376389\n",
      "train loss:1.4651173801565114\n",
      "train loss:1.2790870641627263\n",
      "train loss:1.1728182737260116\n",
      "train loss:1.3486911998859525\n",
      "train loss:1.181750401619146\n",
      "train loss:1.417437797923386\n",
      "train loss:1.3760954385189565\n",
      "train loss:1.295610937441414\n",
      "train loss:1.1912506003986776\n",
      "train loss:1.4649417821789141\n",
      "train loss:1.2945953238154906\n",
      "train loss:1.1257508138514563\n",
      "train loss:1.3172938100151805\n",
      "train loss:1.3141182300469652\n",
      "train loss:1.2398534271904886\n",
      "train loss:1.0417850653828127\n",
      "train loss:1.3611982820389823\n",
      "train loss:1.2383177508612695\n",
      "train loss:1.2653095930122298\n",
      "train loss:1.186718185448439\n",
      "train loss:1.2885976233738219\n",
      "train loss:1.2253485873294385\n",
      "train loss:1.2994937971801743\n",
      "train loss:1.2046989054049808\n",
      "train loss:1.2643650106197868\n",
      "train loss:1.1177685999109441\n",
      "train loss:1.2028597582161735\n",
      "train loss:1.2140991310189961\n",
      "train loss:1.2755496913957236\n",
      "train loss:1.1950352954456374\n",
      "train loss:1.1456967527006836\n",
      "train loss:1.2097674122669393\n",
      "train loss:1.0776890693183683\n",
      "train loss:1.1138316817479448\n",
      "train loss:1.176671810907684\n",
      "train loss:1.3719196390526642\n",
      "train loss:1.2584775595100268\n",
      "train loss:1.4548261156151525\n",
      "train loss:1.346111701990946\n",
      "train loss:1.2758904216602986\n",
      "train loss:1.1703821370118188\n",
      "train loss:1.1875979341544014\n",
      "train loss:1.3762372489128445\n",
      "train loss:1.137491106955277\n",
      "train loss:1.2771833275851234\n",
      "train loss:1.1824119716487873\n",
      "=== epoch:6, train acc:0.967, test acc:0.954 ===\n",
      "train loss:1.381536861510935\n",
      "train loss:1.1428464301550056\n",
      "train loss:1.306815721866953\n",
      "train loss:1.339946340219295\n",
      "train loss:1.3249008916456715\n",
      "train loss:1.3031322377401224\n",
      "train loss:1.1855324372263742\n",
      "train loss:1.2398624324268541\n",
      "train loss:1.1254860498657964\n",
      "train loss:1.1658244271398899\n",
      "train loss:1.1954576467172673\n",
      "train loss:1.228185740909286\n",
      "train loss:1.2687568605757145\n",
      "train loss:1.3317781133974353\n",
      "train loss:1.1496956961329545\n",
      "train loss:1.1907913804546915\n",
      "train loss:1.1573512646095774\n",
      "train loss:1.2123570169679032\n",
      "train loss:0.9846491997628398\n",
      "train loss:1.1748714239115194\n",
      "train loss:1.2172940437588853\n",
      "train loss:1.1714099325734026\n",
      "train loss:1.1140134053096649\n",
      "train loss:1.0770923538222361\n",
      "train loss:1.2159661086902191\n",
      "train loss:1.086155402374824\n",
      "train loss:1.1125511173648384\n",
      "train loss:1.2389570902826272\n",
      "train loss:1.2630688355072521\n",
      "train loss:1.1970383884291467\n",
      "train loss:1.0361129441893835\n",
      "train loss:1.0885088543554744\n",
      "train loss:1.191128319442329\n",
      "train loss:1.3330864906993058\n",
      "train loss:1.1311539026030573\n",
      "train loss:1.23754881960029\n",
      "train loss:1.3431079796086207\n",
      "train loss:1.2260719268522653\n",
      "train loss:1.210316367692939\n",
      "train loss:1.110333014511741\n",
      "train loss:1.2157026009909235\n",
      "train loss:1.1349819069382876\n",
      "train loss:1.2026712767555414\n",
      "train loss:1.1045857862166308\n",
      "train loss:1.0873569490570032\n",
      "train loss:1.1741972653009887\n",
      "train loss:1.0259350287647897\n",
      "train loss:1.1402009623390894\n",
      "train loss:1.244108407766282\n",
      "train loss:1.0603195581555172\n",
      "=== epoch:7, train acc:0.97, test acc:0.953 ===\n",
      "train loss:0.9642511836399607\n",
      "train loss:1.1410053930530788\n",
      "train loss:1.0962909847918003\n",
      "train loss:1.1830992313374558\n",
      "train loss:1.2320671318546532\n",
      "train loss:1.208874343777434\n",
      "train loss:1.1307168461316213\n",
      "train loss:1.187700574364068\n",
      "train loss:1.0298333163232372\n",
      "train loss:1.282308344898151\n",
      "train loss:1.105651819696558\n",
      "train loss:1.0699793088039837\n",
      "train loss:1.2723766588428789\n",
      "train loss:1.1867677257474796\n",
      "train loss:1.2430352888816472\n",
      "train loss:1.1782884140528749\n",
      "train loss:1.1598351444999704\n",
      "train loss:1.198050251601289\n",
      "train loss:1.2552608302261365\n",
      "train loss:1.3128928748220139\n",
      "train loss:1.1704023855970296\n",
      "train loss:1.2190660179524098\n",
      "train loss:1.3012806095168694\n",
      "train loss:1.253750317981459\n",
      "train loss:1.3614954630992289\n",
      "train loss:1.2063718537467665\n",
      "train loss:1.1711338175118577\n",
      "train loss:1.1613035720651919\n",
      "train loss:1.2566530488949643\n",
      "train loss:0.9619124812703492\n",
      "train loss:1.2277602183864347\n",
      "train loss:1.1945727472825132\n",
      "train loss:1.1486429134983769\n",
      "train loss:1.3150617404310927\n",
      "train loss:1.1723652830116889\n",
      "train loss:1.2670876643727367\n",
      "train loss:1.0918550545612036\n",
      "train loss:1.1870152135159635\n",
      "train loss:1.0372897415622817\n",
      "train loss:1.2594389975224396\n",
      "train loss:1.1154219471171798\n",
      "train loss:1.2312068578130038\n",
      "train loss:1.1025592591086812\n",
      "train loss:1.2104644036931993\n",
      "train loss:1.1335093544763786\n",
      "train loss:1.2657374597939484\n",
      "train loss:1.1711900999565947\n",
      "train loss:1.1366973703616856\n",
      "train loss:1.1692038375563534\n",
      "train loss:1.2355388498688595\n",
      "=== epoch:8, train acc:0.969, test acc:0.955 ===\n",
      "train loss:1.1263993746545382\n",
      "train loss:1.078863838329467\n",
      "train loss:1.0941304233142042\n",
      "train loss:1.2708390306181163\n",
      "train loss:1.2987196187937795\n",
      "train loss:1.1254355043294553\n",
      "train loss:1.209273224435219\n",
      "train loss:1.2925746871119486\n",
      "train loss:0.9391023879959485\n",
      "train loss:0.9696934675983875\n",
      "train loss:1.146718246332114\n",
      "train loss:1.2594947664544554\n",
      "train loss:1.1068447436854925\n",
      "train loss:1.2070617644046835\n",
      "train loss:1.0885045014094672\n",
      "train loss:1.082038492945763\n",
      "train loss:1.1069797607211422\n",
      "train loss:1.0557220778947674\n",
      "train loss:1.1887385679577163\n",
      "train loss:1.0350625953693637\n",
      "train loss:1.0885368786747578\n",
      "train loss:1.0954675976289243\n",
      "train loss:1.1194083077644945\n",
      "train loss:1.041197768156438\n",
      "train loss:1.0285569432674808\n",
      "train loss:1.0346525536295594\n",
      "train loss:1.28127630754138\n",
      "train loss:1.119267584432004\n",
      "train loss:1.0087025389958928\n",
      "train loss:1.095463040509044\n",
      "train loss:1.0050441443437623\n",
      "train loss:1.1869999521321777\n",
      "train loss:1.1816707746357098\n",
      "train loss:1.2123685077096402\n",
      "train loss:1.0817663574305894\n",
      "train loss:1.248956402317338\n",
      "train loss:1.177757832593384\n",
      "train loss:0.9402759417289652\n",
      "train loss:1.1707500218835052\n",
      "train loss:1.185892576222054\n",
      "train loss:1.0900268020027304\n",
      "train loss:1.3100638818698942\n",
      "train loss:1.0167297176200008\n",
      "train loss:1.2440894043746729\n",
      "train loss:0.9631465340618345\n",
      "train loss:1.3422682168310613\n",
      "train loss:1.058186371221368\n",
      "train loss:1.0163272801060916\n",
      "train loss:1.0470762122021764\n",
      "train loss:1.3486164783874608\n",
      "=== epoch:9, train acc:0.978, test acc:0.952 ===\n",
      "train loss:1.02144340666124\n",
      "train loss:1.0040825043271633\n",
      "train loss:1.1152094371715333\n",
      "train loss:1.1217835121167647\n",
      "train loss:1.0323355695406162\n",
      "train loss:1.0703311807241707\n",
      "train loss:1.1652089967364447\n",
      "train loss:1.1855417190769413\n",
      "train loss:1.2009415091324902\n",
      "train loss:1.220119711406433\n",
      "train loss:0.975917358746747\n",
      "train loss:1.1648503041105085\n",
      "train loss:1.044534732645687\n",
      "train loss:1.1496300846922316\n",
      "train loss:1.1666219803358318\n",
      "train loss:1.0441351778333292\n",
      "train loss:0.9362812437508574\n",
      "train loss:1.0545515040156133\n",
      "train loss:1.0865962834680079\n",
      "train loss:0.9879393636000934\n",
      "train loss:1.3237715901329776\n",
      "train loss:1.1013691014224363\n",
      "train loss:1.0310226847851423\n",
      "train loss:1.0753577513238504\n",
      "train loss:1.117181560962858\n",
      "train loss:1.3232334469331613\n",
      "train loss:1.1581158598672765\n",
      "train loss:1.1049597360464076\n",
      "train loss:1.1607113912716391\n",
      "train loss:1.236584164146118\n",
      "train loss:1.339345007070144\n",
      "train loss:1.071183565764538\n",
      "train loss:1.073599207877726\n",
      "train loss:1.1752618538711586\n",
      "train loss:1.201860403428602\n",
      "train loss:1.1485089449307697\n",
      "train loss:1.0162454399593097\n",
      "train loss:1.0586892142793394\n",
      "train loss:1.037600092585861\n",
      "train loss:1.0446822427796896\n",
      "train loss:1.041340812754025\n",
      "train loss:1.0766178977816347\n",
      "train loss:1.067631077314885\n",
      "train loss:1.0427433063381504\n",
      "train loss:1.0356545810767617\n",
      "train loss:1.0214427060497728\n",
      "train loss:1.1277806167985147\n",
      "train loss:1.0239949470461638\n",
      "train loss:0.9335355166194251\n",
      "train loss:1.0907617722089098\n",
      "=== epoch:10, train acc:0.978, test acc:0.966 ===\n",
      "train loss:1.18905249270232\n",
      "train loss:0.9267749230558114\n",
      "train loss:0.9590678284469429\n",
      "train loss:0.9613995983342192\n",
      "train loss:1.2550382960754423\n",
      "train loss:1.0739777876128367\n",
      "train loss:1.0938414761649684\n",
      "train loss:1.190546651478425\n",
      "train loss:1.0860823389263126\n",
      "train loss:1.243254472430043\n",
      "train loss:0.9185465224027327\n",
      "train loss:1.0295546645555178\n",
      "train loss:1.0219037753530367\n",
      "train loss:0.8898923924949126\n",
      "train loss:1.1068662936149078\n",
      "train loss:1.130974053541837\n",
      "train loss:1.1809334149742619\n",
      "train loss:1.153062981239948\n",
      "train loss:1.3257829080819379\n",
      "train loss:1.2498807348934158\n",
      "train loss:1.069220731664811\n",
      "train loss:1.1064651886518133\n",
      "train loss:1.051860725384481\n",
      "train loss:1.1353155280176401\n",
      "train loss:0.9705312802289713\n",
      "train loss:1.0641158674718465\n",
      "train loss:1.049934390278193\n",
      "train loss:1.0493672279216877\n",
      "train loss:1.092074174673871\n",
      "train loss:1.0873462133310463\n",
      "train loss:1.0279301674423675\n",
      "train loss:1.205187797143372\n",
      "train loss:1.136072274609664\n",
      "train loss:1.1128838562820595\n",
      "train loss:1.0969349506931891\n",
      "train loss:0.9755678958590477\n",
      "train loss:0.9621454151998476\n",
      "train loss:1.2451069904748466\n",
      "train loss:1.143519656819876\n",
      "train loss:1.2278872890832462\n",
      "train loss:0.8653042803561423\n",
      "train loss:0.9708851156051946\n",
      "train loss:0.9645400657733095\n",
      "train loss:1.0893036914918979\n",
      "train loss:1.0837692109308343\n",
      "train loss:0.9661019125072079\n",
      "train loss:1.1502415606156366\n",
      "train loss:0.9530882901853377\n",
      "train loss:1.202799561865404\n",
      "train loss:1.0513133919809936\n",
      "=== epoch:11, train acc:0.982, test acc:0.965 ===\n",
      "train loss:1.079523984062833\n",
      "train loss:0.9386009713732328\n",
      "train loss:1.0744360189537254\n",
      "train loss:1.051518320403325\n",
      "train loss:0.967472072966547\n",
      "train loss:1.2074869407659004\n",
      "train loss:1.0736702818800543\n",
      "train loss:1.2660349567013727\n",
      "train loss:0.9972951437610766\n",
      "train loss:0.9194110832647597\n",
      "train loss:1.1403455171398407\n",
      "train loss:0.9765587794784445\n",
      "train loss:1.040330778811525\n",
      "train loss:1.1695385459990346\n",
      "train loss:1.2260688300123705\n",
      "train loss:0.9512335902790121\n",
      "train loss:1.059297515538753\n",
      "train loss:1.1998714878549366\n",
      "train loss:1.0214226110357045\n",
      "train loss:0.8149366581615968\n",
      "train loss:0.9901952538042046\n",
      "train loss:1.0339682539947963\n",
      "train loss:1.141353021995532\n",
      "train loss:1.076337138808538\n",
      "train loss:0.9249078187339984\n",
      "train loss:1.1089342105242153\n",
      "train loss:1.3616676908251397\n",
      "train loss:1.2713569194802163\n",
      "train loss:1.1175053463132654\n",
      "train loss:1.1001682102228383\n",
      "train loss:0.9662952565929024\n",
      "train loss:1.1918097020387863\n",
      "train loss:1.0131826764457488\n",
      "train loss:1.1086469789122508\n",
      "train loss:1.0622579825892908\n",
      "train loss:0.9713664759159422\n",
      "train loss:1.0229213759316576\n",
      "train loss:0.9111434707978474\n",
      "train loss:1.0983224258340147\n",
      "train loss:0.9719955753499941\n",
      "train loss:0.9534064341281611\n",
      "train loss:0.91321041719299\n",
      "train loss:1.0520316770066678\n",
      "train loss:1.1422067423729834\n",
      "train loss:1.0755247303791835\n",
      "train loss:1.055988016291906\n",
      "train loss:0.9870547050945859\n",
      "train loss:1.1890421753286384\n",
      "train loss:1.3173560546875427\n",
      "train loss:1.0989141738932098\n",
      "=== epoch:12, train acc:0.983, test acc:0.965 ===\n",
      "train loss:1.1383845957376393\n",
      "train loss:0.9953236096141634\n",
      "train loss:1.1529306820444312\n",
      "train loss:1.0079920820565886\n",
      "train loss:1.1249919016466452\n",
      "train loss:0.9027441039610749\n",
      "train loss:0.9093296516464201\n",
      "train loss:1.3037821454634904\n",
      "train loss:0.9991741332153413\n",
      "train loss:1.0501781444579774\n",
      "train loss:0.9467006771176861\n",
      "train loss:1.0378560891393613\n",
      "train loss:0.8866055018230472\n",
      "train loss:1.02030057966573\n",
      "train loss:1.0758197281067297\n",
      "train loss:1.0800858138945881\n",
      "train loss:0.9544365727364736\n",
      "train loss:1.1666241752471471\n",
      "train loss:1.0921969254726358\n",
      "train loss:0.9558415487298252\n",
      "train loss:1.0299074039141871\n",
      "train loss:0.9554997795519263\n",
      "train loss:0.8328843665673368\n",
      "train loss:1.005725026111296\n",
      "train loss:0.8319515782607452\n",
      "train loss:1.096177740588759\n",
      "train loss:0.996176750288027\n",
      "train loss:1.0016486338857562\n",
      "train loss:1.0292870445754394\n",
      "train loss:0.9098492905052318\n",
      "train loss:0.9757321423866806\n",
      "train loss:1.01719232115124\n",
      "train loss:1.124007856015692\n",
      "train loss:1.1157556159101718\n",
      "train loss:1.0607799143869008\n",
      "train loss:0.9842665236802951\n",
      "train loss:1.1167022480904862\n",
      "train loss:1.054085303859343\n",
      "train loss:0.9376005728138169\n",
      "train loss:1.122057965016747\n",
      "train loss:1.1219319355446293\n",
      "train loss:0.861441137418847\n",
      "train loss:0.9841061221923826\n",
      "train loss:0.8883851283157428\n",
      "train loss:1.0677540605668911\n",
      "train loss:1.0644905008104877\n",
      "train loss:1.0930032449558056\n",
      "train loss:1.1387571019836435\n",
      "train loss:1.213388385110742\n",
      "train loss:1.048323616463791\n",
      "=== epoch:13, train acc:0.978, test acc:0.962 ===\n",
      "train loss:0.9676713845615049\n",
      "train loss:1.1218017483790417\n",
      "train loss:0.9423080787543741\n",
      "train loss:1.106223852095876\n",
      "train loss:1.0936966368873562\n",
      "train loss:1.128785568336997\n",
      "train loss:1.2023873464099142\n",
      "train loss:1.1240659051952282\n",
      "train loss:1.1709627107523914\n",
      "train loss:1.2027177747478008\n",
      "train loss:1.0972769021978344\n",
      "train loss:0.8934804793352888\n",
      "train loss:1.0929581858138158\n",
      "train loss:0.8980477072885265\n",
      "train loss:1.1181168407615514\n",
      "train loss:0.9990473313849974\n",
      "train loss:0.8532938611935752\n",
      "train loss:0.9811931399454802\n",
      "train loss:1.1591515374971966\n",
      "train loss:1.1426812738844392\n",
      "train loss:1.0541723965500909\n",
      "train loss:1.031894770174677\n",
      "train loss:0.8760135037928538\n",
      "train loss:1.0186795958856363\n",
      "train loss:0.9505828903259222\n",
      "train loss:1.0860789224872207\n",
      "train loss:1.1343825175507263\n",
      "train loss:0.8696677897544092\n",
      "train loss:1.1257027894149727\n",
      "train loss:0.9486387678638042\n",
      "train loss:1.1437347974693726\n",
      "train loss:0.9652003415056208\n",
      "train loss:1.1073363486105103\n",
      "train loss:1.0377426954634972\n",
      "train loss:1.0779490366474571\n",
      "train loss:0.983511026668215\n",
      "train loss:0.9256853274512299\n",
      "train loss:0.9030996070306532\n",
      "train loss:0.9826107322454363\n",
      "train loss:1.1685996450585332\n",
      "train loss:1.173237399454618\n",
      "train loss:1.012011190553919\n",
      "train loss:1.270444087059341\n",
      "train loss:1.0603676707261487\n",
      "train loss:0.9382544823975729\n",
      "train loss:1.0567872239590617\n",
      "train loss:1.023632543315136\n",
      "train loss:0.9510971358196895\n",
      "train loss:1.236004497191582\n",
      "train loss:1.0120501389555658\n",
      "=== epoch:14, train acc:0.989, test acc:0.968 ===\n",
      "train loss:1.0432111317986963\n",
      "train loss:0.9453968345912104\n",
      "train loss:1.0458472024230472\n",
      "train loss:1.1008167218251594\n",
      "train loss:1.0434521604613025\n",
      "train loss:1.0551406581338698\n",
      "train loss:0.8927370375060916\n",
      "train loss:1.0748549072092612\n",
      "train loss:0.8085229028033403\n",
      "train loss:0.8763521853000171\n",
      "train loss:1.0175832202914177\n",
      "train loss:1.0043337058667787\n",
      "train loss:1.092675539084221\n",
      "train loss:1.0740344348342565\n",
      "train loss:1.047697632011417\n",
      "train loss:1.065359570093258\n",
      "train loss:1.1813640208734657\n",
      "train loss:1.079377551083497\n",
      "train loss:1.1000194643670056\n",
      "train loss:1.0597996958602922\n",
      "train loss:0.8761155263308206\n",
      "train loss:0.9227680599809877\n",
      "train loss:1.063791393194637\n",
      "train loss:1.1055327552445409\n",
      "train loss:1.0057821059180332\n",
      "train loss:0.9612870054763458\n",
      "train loss:1.0204843626116264\n",
      "train loss:0.8917279879120049\n",
      "train loss:1.082629630842838\n",
      "train loss:0.9675283372775496\n",
      "train loss:1.0719739739039695\n",
      "train loss:1.1257900149302447\n",
      "train loss:1.1173936022875888\n",
      "train loss:0.96008187787879\n",
      "train loss:0.8756736137635809\n",
      "train loss:1.0027595306404558\n",
      "train loss:0.9527222038592617\n",
      "train loss:0.8600869103017191\n",
      "train loss:1.1030855181097008\n",
      "train loss:1.0548861426877763\n",
      "train loss:1.0494108470459942\n",
      "train loss:1.116777998711153\n",
      "train loss:0.8848221538811144\n",
      "train loss:1.085832540605061\n",
      "train loss:1.0515850803723696\n",
      "train loss:1.026443764639371\n",
      "train loss:1.2343481779030467\n",
      "train loss:0.8390268424141272\n",
      "train loss:0.986122278219718\n",
      "train loss:0.9280267359079954\n",
      "=== epoch:15, train acc:0.988, test acc:0.975 ===\n",
      "train loss:1.0177283011786151\n",
      "train loss:0.9853607627595607\n",
      "train loss:1.0817467728014676\n",
      "train loss:1.095659298166836\n",
      "train loss:1.1782515990169562\n",
      "train loss:1.202158580390924\n",
      "train loss:1.0578217469864415\n",
      "train loss:0.9622297288212697\n",
      "train loss:0.963596101496359\n",
      "train loss:0.8808467010473033\n",
      "train loss:1.1372279184341672\n",
      "train loss:1.0261982569301151\n",
      "train loss:0.9692207774354621\n",
      "train loss:0.9456773284620148\n",
      "train loss:1.1907207970810276\n",
      "train loss:1.0310951051660364\n",
      "train loss:0.9395207591339555\n",
      "train loss:1.0666109776747226\n",
      "train loss:0.9893086036735022\n",
      "train loss:0.9684965393374746\n",
      "train loss:1.0347510255619865\n",
      "train loss:0.7714668282218363\n",
      "train loss:0.8964823635054502\n",
      "train loss:0.9404576143471816\n",
      "train loss:1.1258385978872183\n",
      "train loss:1.1008964267736399\n",
      "train loss:0.9164554605027346\n",
      "train loss:0.9973338312525541\n",
      "train loss:0.8724644328016048\n",
      "train loss:0.9935532142943346\n",
      "train loss:0.9484765403176287\n",
      "train loss:0.9756764919001227\n",
      "train loss:1.0519191579531098\n",
      "train loss:0.881526794428659\n",
      "train loss:1.0803039402399803\n",
      "train loss:1.1977601290966746\n",
      "train loss:0.9402090842375317\n",
      "train loss:1.1015392733840428\n",
      "train loss:1.0435427855794825\n",
      "train loss:0.912264438537871\n",
      "train loss:1.0365745824133006\n",
      "train loss:0.9868433289879012\n",
      "train loss:1.158640184796314\n",
      "train loss:0.9074252060544652\n",
      "train loss:0.9337535170694803\n",
      "train loss:1.1237006176318578\n",
      "train loss:0.89572117214408\n",
      "train loss:1.0701306954375256\n",
      "train loss:0.9128292678216582\n",
      "train loss:1.1490227015607948\n",
      "=== epoch:16, train acc:0.991, test acc:0.967 ===\n",
      "train loss:1.0187791684675809\n",
      "train loss:1.2189126924006366\n",
      "train loss:0.9314118104811768\n",
      "train loss:0.9360924158277435\n",
      "train loss:1.0277633429594013\n",
      "train loss:0.89870220488143\n",
      "train loss:1.101563729026258\n",
      "train loss:1.044768566136358\n",
      "train loss:1.07477647185759\n",
      "train loss:1.0918267722871486\n",
      "train loss:0.9087788012502928\n",
      "train loss:1.245917422533501\n",
      "train loss:0.8299604043687623\n",
      "train loss:0.9075057416194424\n",
      "train loss:1.0565386949265747\n",
      "train loss:0.9926175439477012\n",
      "train loss:1.00922456626581\n",
      "train loss:0.9347474506357076\n",
      "train loss:0.863786997363142\n",
      "train loss:0.9574431558724907\n",
      "train loss:1.031877454723024\n",
      "train loss:0.8454334327635905\n",
      "train loss:1.2445945041220796\n",
      "train loss:1.0437278709689808\n",
      "train loss:0.8634476547667091\n",
      "train loss:0.9867550666829658\n",
      "train loss:0.9335028285972409\n",
      "train loss:0.9582603616796326\n",
      "train loss:1.1341029099044773\n",
      "train loss:0.8984133270972009\n",
      "train loss:1.180885529599766\n",
      "train loss:0.8963509830835339\n",
      "train loss:1.0659911037006768\n",
      "train loss:1.0695235714628617\n",
      "train loss:1.1744381068031362\n",
      "train loss:1.0699970142590658\n",
      "train loss:1.0825558173260148\n",
      "train loss:1.0361130410786465\n",
      "train loss:0.9625940188693217\n",
      "train loss:1.0118636953684308\n",
      "train loss:0.8737813287224392\n",
      "train loss:1.0505359251644224\n",
      "train loss:0.8721517080723318\n",
      "train loss:0.9095874283592335\n",
      "train loss:0.8784716391815741\n",
      "train loss:0.9301687580157261\n",
      "train loss:1.0877491285596947\n",
      "train loss:0.9504198378387544\n",
      "train loss:1.0557187068734648\n",
      "train loss:0.9009484571235946\n",
      "=== epoch:17, train acc:0.992, test acc:0.972 ===\n",
      "train loss:1.1337007814568765\n",
      "train loss:0.9903462501511759\n",
      "train loss:1.005259721251002\n",
      "train loss:0.9247334856757135\n",
      "train loss:1.087756298857757\n",
      "train loss:0.9049992897292394\n",
      "train loss:1.066657899709258\n",
      "train loss:1.065857489827269\n",
      "train loss:1.0655354019070953\n",
      "train loss:1.0695987127138273\n",
      "train loss:1.107380933782281\n",
      "train loss:0.8765821868819347\n",
      "train loss:1.0373275578756902\n",
      "train loss:1.0319573246498184\n",
      "train loss:0.9233325166632007\n",
      "train loss:1.1034994528618798\n",
      "train loss:1.0488500742003009\n",
      "train loss:0.8577980643394667\n",
      "train loss:0.8553286370168294\n",
      "train loss:0.9867315999196705\n",
      "train loss:1.3584301661736182\n",
      "train loss:0.8498955249264736\n",
      "train loss:1.023276809967478\n",
      "train loss:0.9294245833941531\n",
      "train loss:0.9880774254786624\n",
      "train loss:0.9696992027024381\n",
      "train loss:1.0933156775469814\n",
      "train loss:0.7112835106080504\n",
      "train loss:0.9150192686618789\n",
      "train loss:0.9755497152557424\n",
      "train loss:1.2013407643972043\n",
      "train loss:1.1240665173017588\n",
      "train loss:0.9866887592435419\n",
      "train loss:0.9518336509555558\n",
      "train loss:0.8398675121656811\n",
      "train loss:0.8428807853863189\n",
      "train loss:1.1117900535503404\n",
      "train loss:0.8709725042958961\n",
      "train loss:0.9065040951573344\n",
      "train loss:1.2007847289807523\n",
      "train loss:1.0349512224652764\n",
      "train loss:0.8738021179958779\n",
      "train loss:1.0409815124616153\n",
      "train loss:1.2071043077771215\n",
      "train loss:1.132712668401361\n",
      "train loss:0.9115201832785658\n",
      "train loss:0.9348949820879959\n",
      "train loss:0.9285183705781779\n",
      "train loss:1.2360092646139555\n",
      "train loss:0.8654736441771977\n",
      "=== epoch:18, train acc:0.989, test acc:0.97 ===\n",
      "train loss:0.9272567439384158\n",
      "train loss:1.004318631867635\n",
      "train loss:0.8880328408433666\n",
      "train loss:0.9909511693200543\n",
      "train loss:0.809531238493774\n",
      "train loss:1.0284804854257066\n",
      "train loss:1.1030159701036768\n",
      "train loss:0.6990307183898474\n",
      "train loss:0.8770872767115627\n",
      "train loss:1.039659780058418\n",
      "train loss:1.0321806496771495\n",
      "train loss:0.9328804086649397\n",
      "train loss:0.8362023126098618\n",
      "train loss:0.8268938467947095\n",
      "train loss:0.9363964035694472\n",
      "train loss:0.7511371256087042\n",
      "train loss:1.1385300837641323\n",
      "train loss:1.0856154003789469\n",
      "train loss:1.1266010696934528\n",
      "train loss:1.0185787293917956\n",
      "train loss:0.9989023197490059\n",
      "train loss:1.2536435022426355\n",
      "train loss:0.9291490577977203\n",
      "train loss:1.0124567015127808\n",
      "train loss:0.9945014791316511\n",
      "train loss:1.011095433949323\n",
      "train loss:0.762024590258904\n",
      "train loss:1.024919146780541\n",
      "train loss:0.9964653311279263\n",
      "train loss:0.8240562207277528\n",
      "train loss:0.9268254861437067\n",
      "train loss:1.1835953240514419\n",
      "train loss:1.0182345186586532\n",
      "train loss:0.9876976532120655\n",
      "train loss:0.9899273591547926\n",
      "train loss:1.0681780591958898\n",
      "train loss:0.8508615442136598\n",
      "train loss:0.9696409847719042\n",
      "train loss:0.8711949854105678\n",
      "train loss:1.0725563724699854\n",
      "train loss:0.9115747672157866\n",
      "train loss:0.9421884699818932\n",
      "train loss:0.9505533959667178\n",
      "train loss:1.0489521623340916\n",
      "train loss:1.0459066858059716\n",
      "train loss:0.9532621340881959\n",
      "train loss:0.965703274242124\n",
      "train loss:0.8243863128426975\n",
      "train loss:1.1495233432847018\n",
      "train loss:0.9925418710758206\n",
      "=== epoch:19, train acc:0.996, test acc:0.976 ===\n",
      "train loss:1.0634421107359004\n",
      "train loss:1.113802168216651\n",
      "train loss:0.8933103296737644\n",
      "train loss:0.9511784485127085\n",
      "train loss:1.0089749926889287\n",
      "train loss:1.0874691881489336\n",
      "train loss:0.9198575635008716\n",
      "train loss:1.0366666196031782\n",
      "train loss:1.0349689199837337\n",
      "train loss:1.0936466483083664\n",
      "train loss:1.0149431226048005\n",
      "train loss:0.8327988359851628\n",
      "train loss:1.1719335674459328\n",
      "train loss:0.9682898731065572\n",
      "train loss:0.9722314973922672\n",
      "train loss:1.029405421238138\n",
      "train loss:1.0120441273734375\n",
      "train loss:0.727699201010908\n",
      "train loss:1.027500491671445\n",
      "train loss:1.0097239051632882\n",
      "train loss:1.0150544456299009\n",
      "train loss:1.0201078196217557\n",
      "train loss:0.8423315831968934\n",
      "train loss:0.8247242138278412\n",
      "train loss:1.0058266000175158\n",
      "train loss:0.7945520053604621\n",
      "train loss:0.9018576280703055\n",
      "train loss:1.1348158652344964\n",
      "train loss:1.1100384566893333\n",
      "train loss:1.0354530831358777\n",
      "train loss:0.9040395626184713\n",
      "train loss:1.0959319593944492\n",
      "train loss:1.108228933004395\n",
      "train loss:1.0544055256548048\n",
      "train loss:1.0001654724308002\n",
      "train loss:0.9413707592081022\n",
      "train loss:1.0521402907584372\n",
      "train loss:0.8875607262493364\n",
      "train loss:0.9912677987502905\n",
      "train loss:1.0927235560693793\n",
      "train loss:0.9212413391622433\n",
      "train loss:1.1005403080615346\n",
      "train loss:1.0107411127057069\n",
      "train loss:1.006539403227448\n",
      "train loss:0.958422609873966\n",
      "train loss:1.0428986319910662\n",
      "train loss:1.0718250726893546\n",
      "train loss:1.1522743918489993\n",
      "train loss:0.9713575602991313\n",
      "train loss:0.9932699578460784\n",
      "=== epoch:20, train acc:0.993, test acc:0.98 ===\n",
      "train loss:0.8701139627190853\n",
      "train loss:0.9610959560864518\n",
      "train loss:1.235747580780081\n",
      "train loss:0.8918237849807846\n",
      "train loss:1.0779533578652325\n",
      "train loss:1.041316636067443\n",
      "train loss:0.9014050317084287\n",
      "train loss:1.029176730905676\n",
      "train loss:1.0591109256531916\n",
      "train loss:0.9825177979438314\n",
      "train loss:0.8928903671882186\n",
      "train loss:1.1591550081700854\n",
      "train loss:1.090105633468465\n",
      "train loss:1.008071888146803\n",
      "train loss:0.8817380137522224\n",
      "train loss:0.9078645278121528\n",
      "train loss:0.9830086960931418\n",
      "train loss:1.1015667963695408\n",
      "train loss:0.9223866362187252\n",
      "train loss:1.0487596989016403\n",
      "train loss:0.8665642038277609\n",
      "train loss:0.9468436531304871\n",
      "train loss:1.0063627541519948\n",
      "train loss:1.0103925727540701\n",
      "train loss:1.0056390603307261\n",
      "train loss:0.8504785495938022\n",
      "train loss:0.9730102811011028\n",
      "train loss:1.020796165905542\n",
      "train loss:1.0692095799597572\n",
      "train loss:1.0960114255991116\n",
      "train loss:0.9405202378186145\n",
      "train loss:1.1428665327037266\n",
      "train loss:1.0053241773570896\n",
      "train loss:1.1092166995469532\n",
      "train loss:0.9477040555404337\n",
      "train loss:0.8480556657511564\n",
      "train loss:1.060237587889524\n",
      "train loss:0.8998748981984944\n",
      "train loss:0.984352533446738\n",
      "train loss:0.8816858069228226\n",
      "train loss:0.8477789303983514\n",
      "train loss:1.1134851630510987\n",
      "train loss:1.0231114877760676\n",
      "train loss:1.0543857139179353\n",
      "train loss:0.8450182821123079\n",
      "train loss:0.8685733609639433\n",
      "train loss:1.012708971703897\n",
      "train loss:0.8995403142515468\n",
      "train loss:0.9473799017794139\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.977\n",
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from data.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
