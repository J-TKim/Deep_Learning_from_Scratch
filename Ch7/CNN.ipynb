{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7) # (데이터 수, 채널 수, 높이, 너비)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7) # 데이터 10개\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(Fn, -1).T # 필터 전개\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 최댓값 (2)\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        # 성형 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0],\n",
    "                                                             filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "            \n",
    "    \n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29974200475505\n",
      "=== epoch:1, train acc:0.237, test acc:0.234 ===\n",
      "train loss:2.2974697703435605\n",
      "train loss:2.292874772119707\n",
      "train loss:2.2833303215715244\n",
      "train loss:2.2771828512293495\n",
      "train loss:2.268662423814519\n",
      "train loss:2.2472225916705693\n",
      "train loss:2.234853294254394\n",
      "train loss:2.2174614674302724\n",
      "train loss:2.1496421331920823\n",
      "train loss:2.140190247541056\n",
      "train loss:2.068080834186786\n",
      "train loss:2.0444212019851777\n",
      "train loss:2.0280771014265166\n",
      "train loss:1.9574804077861747\n",
      "train loss:1.8248363238524263\n",
      "train loss:1.8571324894093473\n",
      "train loss:1.7838040154478665\n",
      "train loss:1.660980129776189\n",
      "train loss:1.5989014829779504\n",
      "train loss:1.4992656956477617\n",
      "train loss:1.4596527146644493\n",
      "train loss:1.4615811948965323\n",
      "train loss:1.2950689271513907\n",
      "train loss:1.1500907282514385\n",
      "train loss:1.1205876755522368\n",
      "train loss:0.9608996414175127\n",
      "train loss:1.036687485250911\n",
      "train loss:0.9020530944498829\n",
      "train loss:0.8745329591681004\n",
      "train loss:0.7389631139500108\n",
      "train loss:0.7843591382065433\n",
      "train loss:0.7931843450659534\n",
      "train loss:0.6951775865792631\n",
      "train loss:0.6721433302235628\n",
      "train loss:0.6752021228652072\n",
      "train loss:0.6847550954198259\n",
      "train loss:0.6283427983483266\n",
      "train loss:0.5655810002372261\n",
      "train loss:0.678753464767595\n",
      "train loss:0.517165739679173\n",
      "train loss:0.5661686902334787\n",
      "train loss:0.6098515568073156\n",
      "train loss:0.6830935123728183\n",
      "train loss:0.5141945015210047\n",
      "train loss:0.5419273250648291\n",
      "train loss:0.39632568384359423\n",
      "train loss:0.36816730356225313\n",
      "train loss:0.3697981788916456\n",
      "train loss:0.4657572975572652\n",
      "train loss:0.5996533058081993\n",
      "=== epoch:2, train acc:0.822, test acc:0.801 ===\n",
      "train loss:0.6919455692395927\n",
      "train loss:0.48440990193417227\n",
      "train loss:0.4456562485583148\n",
      "train loss:0.3558455574587803\n",
      "train loss:0.34532939145582603\n",
      "train loss:0.48609268464927263\n",
      "train loss:0.3434323857246407\n",
      "train loss:0.4233487436717454\n",
      "train loss:0.4338460335112036\n",
      "train loss:0.2760100047336939\n",
      "train loss:0.26355314390200496\n",
      "train loss:0.5813271129250281\n",
      "train loss:0.3850082974644773\n",
      "train loss:0.45680728590488917\n",
      "train loss:0.6070039195825274\n",
      "train loss:0.34195705352800637\n",
      "train loss:0.3318485258178479\n",
      "train loss:0.4873284441098169\n",
      "train loss:0.31402464648187994\n",
      "train loss:0.4827362830000913\n",
      "train loss:0.4760995494460268\n",
      "train loss:0.3340596884180684\n",
      "train loss:0.43083390444355735\n",
      "train loss:0.3314244028272533\n",
      "train loss:0.4268681957666191\n",
      "train loss:0.3998186467123604\n",
      "train loss:0.44724708997559787\n",
      "train loss:0.2668030666239271\n",
      "train loss:0.3446972018667095\n",
      "train loss:0.39263081875586087\n",
      "train loss:0.6949172554012293\n",
      "train loss:0.2471853319006588\n",
      "train loss:0.4216688375803665\n",
      "train loss:0.2521973357502241\n",
      "train loss:0.2733401468981438\n",
      "train loss:0.4195913897829015\n",
      "train loss:0.4688713444564214\n",
      "train loss:0.3009071216726116\n",
      "train loss:0.4782732202655663\n",
      "train loss:0.5316710178980242\n",
      "train loss:0.33835488691509125\n",
      "train loss:0.2982971464674491\n",
      "train loss:0.21972377809436897\n",
      "train loss:0.21905923657306162\n",
      "train loss:0.43743232254357944\n",
      "train loss:0.25369261128328774\n",
      "train loss:0.4073710567367352\n",
      "train loss:0.4106924636557043\n",
      "train loss:0.43800887580194603\n",
      "train loss:0.41811113586403353\n",
      "=== epoch:3, train acc:0.875, test acc:0.854 ===\n",
      "train loss:0.473838529045084\n",
      "train loss:0.39749408048412965\n",
      "train loss:0.24399285871784251\n",
      "train loss:0.2311154145944179\n",
      "train loss:0.2948188069958096\n",
      "train loss:0.20367208708870635\n",
      "train loss:0.282481954006636\n",
      "train loss:0.3325924895066757\n",
      "train loss:0.25170330823539905\n",
      "train loss:0.18943633036439106\n",
      "train loss:0.27595620119112174\n",
      "train loss:0.39264546682738605\n",
      "train loss:0.21178838011094403\n",
      "train loss:0.23073683672608591\n",
      "train loss:0.21216241766107113\n",
      "train loss:0.15939587233275077\n",
      "train loss:0.44717636119408966\n",
      "train loss:0.2632388780560653\n",
      "train loss:0.2264220733223502\n",
      "train loss:0.22381647573412533\n",
      "train loss:0.24654612051803518\n",
      "train loss:0.28970534879579624\n",
      "train loss:0.36052065790378535\n",
      "train loss:0.25736673189318754\n",
      "train loss:0.22776571813918298\n",
      "train loss:0.28295806882107544\n",
      "train loss:0.2908543129584148\n",
      "train loss:0.3378617245537972\n",
      "train loss:0.22522553445548385\n",
      "train loss:0.25847948898925815\n",
      "train loss:0.2810740505424617\n",
      "train loss:0.15231672670817265\n",
      "train loss:0.18733666165281984\n",
      "train loss:0.1887327062211\n",
      "train loss:0.29693759012633203\n",
      "train loss:0.40254586372319084\n",
      "train loss:0.40328544881096173\n",
      "train loss:0.21218683797470259\n",
      "train loss:0.2563698757472711\n",
      "train loss:0.3724579641233891\n",
      "train loss:0.2304859264143612\n",
      "train loss:0.2042360141198961\n",
      "train loss:0.25938557354686487\n",
      "train loss:0.2658852425107162\n",
      "train loss:0.24673772482898432\n",
      "train loss:0.19893495701331496\n",
      "train loss:0.1301059480581799\n",
      "train loss:0.3097785787473813\n",
      "train loss:0.5244579198812165\n",
      "train loss:0.37571607304046944\n",
      "=== epoch:4, train acc:0.911, test acc:0.893 ===\n",
      "train loss:0.2557034472398736\n",
      "train loss:0.3890700808248422\n",
      "train loss:0.2962456357214519\n",
      "train loss:0.14090124015692201\n",
      "train loss:0.29190117323083814\n",
      "train loss:0.16614205718200778\n",
      "train loss:0.32884101347941813\n",
      "train loss:0.14946466598649047\n",
      "train loss:0.229034602574899\n",
      "train loss:0.2336493586315445\n",
      "train loss:0.24444540857071023\n",
      "train loss:0.31683158822835067\n",
      "train loss:0.4506151619045864\n",
      "train loss:0.1890809639415248\n",
      "train loss:0.2863430698394284\n",
      "train loss:0.3096259633462537\n",
      "train loss:0.19296429683854094\n",
      "train loss:0.23476017176787034\n",
      "train loss:0.27846601803079807\n",
      "train loss:0.20360023544672073\n",
      "train loss:0.35642130328829924\n",
      "train loss:0.2888117844433059\n",
      "train loss:0.26286001809461007\n",
      "train loss:0.28242248713637563\n",
      "train loss:0.24040974750896826\n",
      "train loss:0.33769058699853427\n",
      "train loss:0.22441370450562967\n",
      "train loss:0.17798377251902334\n",
      "train loss:0.41165232539694185\n",
      "train loss:0.12498624776611407\n",
      "train loss:0.31824859777151543\n",
      "train loss:0.15749075449701874\n",
      "train loss:0.2595412365711503\n",
      "train loss:0.2553757056448511\n",
      "train loss:0.2694461577767718\n",
      "train loss:0.26227443275820955\n",
      "train loss:0.35996491963961064\n",
      "train loss:0.17889280223205625\n",
      "train loss:0.3714112161408441\n",
      "train loss:0.18971209881897508\n",
      "train loss:0.1747608162128608\n",
      "train loss:0.16501429572367543\n",
      "train loss:0.1992759124522718\n",
      "train loss:0.21023267649083555\n",
      "train loss:0.24921409658346586\n",
      "train loss:0.2400463014615336\n",
      "train loss:0.16303684463310322\n",
      "train loss:0.13698322557989248\n",
      "train loss:0.3607381674868154\n",
      "train loss:0.15657781813210872\n",
      "=== epoch:5, train acc:0.928, test acc:0.91 ===\n",
      "train loss:0.1316347952382798\n",
      "train loss:0.3192912817037038\n",
      "train loss:0.17959712917725784\n",
      "train loss:0.27725905158479963\n",
      "train loss:0.2502015646004336\n",
      "train loss:0.1369925160773896\n",
      "train loss:0.08119368137547062\n",
      "train loss:0.18404235311833436\n",
      "train loss:0.13336050095996996\n",
      "train loss:0.13216485667408656\n",
      "train loss:0.1687953140545872\n",
      "train loss:0.13800805701666263\n",
      "train loss:0.14231269609519553\n",
      "train loss:0.3427452699827864\n",
      "train loss:0.09109409979971167\n",
      "train loss:0.18106753279007015\n",
      "train loss:0.16833493206985445\n",
      "train loss:0.25844736035325555\n",
      "train loss:0.27785164865230644\n",
      "train loss:0.2240547112487646\n",
      "train loss:0.15102734982556398\n",
      "train loss:0.16066779590551117\n",
      "train loss:0.28204220111817707\n",
      "train loss:0.3212186990199599\n",
      "train loss:0.07502634196845738\n",
      "train loss:0.19665561997422054\n",
      "train loss:0.16813396342408113\n",
      "train loss:0.11967923796428515\n",
      "train loss:0.04383111638751029\n",
      "train loss:0.18671638610081426\n",
      "train loss:0.15150052384126222\n",
      "train loss:0.11495679396282561\n",
      "train loss:0.18427987429991877\n",
      "train loss:0.09662967713274022\n",
      "train loss:0.1754497828278905\n",
      "train loss:0.180504624880284\n",
      "train loss:0.27048292508937744\n",
      "train loss:0.10536799667793172\n",
      "train loss:0.1042964696078759\n",
      "train loss:0.1291622708493861\n",
      "train loss:0.11087156039751318\n",
      "train loss:0.10715885691202073\n",
      "train loss:0.10656517635967781\n",
      "train loss:0.22520544010328503\n",
      "train loss:0.17770109729017297\n",
      "train loss:0.10371278323441784\n",
      "train loss:0.2255224174858284\n",
      "train loss:0.14813711437663724\n",
      "train loss:0.20266925614233033\n",
      "train loss:0.1766938543382188\n",
      "=== epoch:6, train acc:0.941, test acc:0.932 ===\n",
      "train loss:0.1510763188977367\n",
      "train loss:0.35249450439358937\n",
      "train loss:0.15931128175881504\n",
      "train loss:0.18930083635927525\n",
      "train loss:0.1722542636535885\n",
      "train loss:0.12373874409372128\n",
      "train loss:0.34291539943332894\n",
      "train loss:0.10539382312452318\n",
      "train loss:0.16899599391543035\n",
      "train loss:0.14783914097633719\n",
      "train loss:0.16386062602148627\n",
      "train loss:0.13317469740868793\n",
      "train loss:0.1291419672194058\n",
      "train loss:0.1609767426049004\n",
      "train loss:0.12506173542538665\n",
      "train loss:0.1568312785375546\n",
      "train loss:0.07294063456590871\n",
      "train loss:0.14523063856575585\n",
      "train loss:0.20391367423871284\n",
      "train loss:0.15092445866255472\n",
      "train loss:0.16832623621238738\n",
      "train loss:0.06778183380588387\n",
      "train loss:0.15551335023940982\n",
      "train loss:0.14255822864154225\n",
      "train loss:0.062254358691818314\n",
      "train loss:0.11111120492024572\n",
      "train loss:0.14338552439075866\n",
      "train loss:0.17942229007982521\n",
      "train loss:0.3149058042592721\n",
      "train loss:0.08896479155069315\n",
      "train loss:0.11353770147175317\n",
      "train loss:0.26120825110458873\n",
      "train loss:0.11580961397859818\n",
      "train loss:0.06759139749842893\n",
      "train loss:0.12129181209487351\n",
      "train loss:0.14776243045406318\n",
      "train loss:0.08381449133121849\n",
      "train loss:0.136243154591319\n",
      "train loss:0.11886442958753336\n",
      "train loss:0.15387440272671418\n",
      "train loss:0.11107009131976409\n",
      "train loss:0.12676579546891859\n",
      "train loss:0.15764544293432606\n",
      "train loss:0.22957234401343168\n",
      "train loss:0.14235999864687857\n",
      "train loss:0.18947144919218697\n",
      "train loss:0.19232674767464666\n",
      "train loss:0.1356306921825605\n",
      "train loss:0.19963699809866672\n",
      "train loss:0.16317091244641216\n",
      "=== epoch:7, train acc:0.942, test acc:0.923 ===\n",
      "train loss:0.16462107042731172\n",
      "train loss:0.06991276322853421\n",
      "train loss:0.0827273837074066\n",
      "train loss:0.07485841691499179\n",
      "train loss:0.10044003990038716\n",
      "train loss:0.13443682717454342\n",
      "train loss:0.247472234800398\n",
      "train loss:0.1777030224914861\n",
      "train loss:0.238697356186044\n",
      "train loss:0.16717150266207534\n",
      "train loss:0.07810166982387978\n",
      "train loss:0.11232211374511036\n",
      "train loss:0.17284386237823707\n",
      "train loss:0.2060169345002165\n",
      "train loss:0.1452254203193907\n",
      "train loss:0.13577995265800502\n",
      "train loss:0.08456481551763317\n",
      "train loss:0.11735970459158032\n",
      "train loss:0.13937668242899753\n",
      "train loss:0.18601768163776047\n",
      "train loss:0.08239902338854643\n",
      "train loss:0.12854265698789244\n",
      "train loss:0.11066852126287702\n",
      "train loss:0.15711290973853687\n",
      "train loss:0.15600987140662206\n",
      "train loss:0.09257794225885756\n",
      "train loss:0.10155382367269349\n",
      "train loss:0.09785592525341158\n",
      "train loss:0.08673353101275696\n",
      "train loss:0.11305179897306827\n",
      "train loss:0.10691332011337645\n",
      "train loss:0.12747226541859447\n",
      "train loss:0.05532373686079689\n",
      "train loss:0.09519078499667531\n",
      "train loss:0.10783136820906485\n",
      "train loss:0.13312545942387188\n",
      "train loss:0.13523353804235902\n",
      "train loss:0.08577821853362806\n",
      "train loss:0.09596715376663612\n",
      "train loss:0.10302995819872393\n",
      "train loss:0.14414525105979903\n",
      "train loss:0.11803752996505215\n",
      "train loss:0.036862348375430745\n",
      "train loss:0.08664429658991293\n",
      "train loss:0.18587439089676214\n",
      "train loss:0.09751092779029297\n",
      "train loss:0.10357500376465416\n",
      "train loss:0.11517276900779538\n",
      "train loss:0.09399784737396341\n",
      "train loss:0.02283480360217351\n",
      "=== epoch:8, train acc:0.957, test acc:0.941 ===\n",
      "train loss:0.3151749236064339\n",
      "train loss:0.13297445709529396\n",
      "train loss:0.08902509641956145\n",
      "train loss:0.16297569152312125\n",
      "train loss:0.12264013509489022\n",
      "train loss:0.08206652475187473\n",
      "train loss:0.04443936935630746\n",
      "train loss:0.10209652378006813\n",
      "train loss:0.09089781972018036\n",
      "train loss:0.08395075614512332\n",
      "train loss:0.2049307756070641\n",
      "train loss:0.12475379897532582\n",
      "train loss:0.19794204947670718\n",
      "train loss:0.04997889897851776\n",
      "train loss:0.0800633457749224\n",
      "train loss:0.11598700076056657\n",
      "train loss:0.10562630980743322\n",
      "train loss:0.09420457894929163\n",
      "train loss:0.05778424131237153\n",
      "train loss:0.11634647701162246\n",
      "train loss:0.14218468883627997\n",
      "train loss:0.09936283522718325\n",
      "train loss:0.13213307213451153\n",
      "train loss:0.1678049033151162\n",
      "train loss:0.0801691641537874\n",
      "train loss:0.09283412211553825\n",
      "train loss:0.06849153840127757\n",
      "train loss:0.0934346032353191\n",
      "train loss:0.10791033056009415\n",
      "train loss:0.0437819035146313\n",
      "train loss:0.14813189512476596\n",
      "train loss:0.053032669992414724\n",
      "train loss:0.04142611880865294\n",
      "train loss:0.19754740432960058\n",
      "train loss:0.10958053422751471\n",
      "train loss:0.1200368727768975\n",
      "train loss:0.1279041713418479\n",
      "train loss:0.08003182240363423\n",
      "train loss:0.10757951494639428\n",
      "train loss:0.036835018215942004\n",
      "train loss:0.1901924549823734\n",
      "train loss:0.1195601285290534\n",
      "train loss:0.13573667953791874\n",
      "train loss:0.06958574599676605\n",
      "train loss:0.05303998247723238\n",
      "train loss:0.13050774801677012\n",
      "train loss:0.09501197920631074\n",
      "train loss:0.1512756823553896\n",
      "train loss:0.15319257269034098\n",
      "train loss:0.05061470339276653\n",
      "=== epoch:9, train acc:0.962, test acc:0.956 ===\n",
      "train loss:0.10412009475484037\n",
      "train loss:0.11462368997385351\n",
      "train loss:0.07291230546756905\n",
      "train loss:0.07564713613490563\n",
      "train loss:0.09858804850700208\n",
      "train loss:0.1918480080389112\n",
      "train loss:0.0946032993135542\n",
      "train loss:0.06526265780663876\n",
      "train loss:0.0772734042255963\n",
      "train loss:0.07487724167933799\n",
      "train loss:0.12598457760957926\n",
      "train loss:0.10375126821085201\n",
      "train loss:0.055421306649946066\n",
      "train loss:0.04395079026154823\n",
      "train loss:0.061929799468478534\n",
      "train loss:0.0905338849531288\n",
      "train loss:0.0751086379618354\n",
      "train loss:0.06507202724494038\n",
      "train loss:0.03402216057080501\n",
      "train loss:0.08326514591587517\n",
      "train loss:0.06571524570839381\n",
      "train loss:0.06382293980795434\n",
      "train loss:0.13498558508350608\n",
      "train loss:0.06010950987417651\n",
      "train loss:0.057426258703483765\n",
      "train loss:0.06055735024421354\n",
      "train loss:0.047560522648194936\n",
      "train loss:0.05738653322838583\n",
      "train loss:0.09898410045138524\n",
      "train loss:0.1295202699689129\n",
      "train loss:0.11208097555024574\n",
      "train loss:0.05862098214305183\n",
      "train loss:0.08113459957993552\n",
      "train loss:0.05506278729236188\n",
      "train loss:0.05853089806860359\n",
      "train loss:0.0994046861770765\n",
      "train loss:0.04958230339343798\n",
      "train loss:0.043434526905794545\n",
      "train loss:0.05196713307577983\n",
      "train loss:0.021008231055648145\n",
      "train loss:0.056070448816489676\n",
      "train loss:0.06822288099625992\n",
      "train loss:0.10253655972802872\n",
      "train loss:0.129648892779711\n",
      "train loss:0.05768132656436171\n",
      "train loss:0.11697302137046674\n",
      "train loss:0.21335262846011205\n",
      "train loss:0.04455173794024536\n",
      "train loss:0.06748435136747684\n",
      "train loss:0.07678633409468992\n",
      "=== epoch:10, train acc:0.964, test acc:0.949 ===\n",
      "train loss:0.034390149281781385\n",
      "train loss:0.11087177005825583\n",
      "train loss:0.1274835476343814\n",
      "train loss:0.07764270605439673\n",
      "train loss:0.05745539099239038\n",
      "train loss:0.12094861853428077\n",
      "train loss:0.031977663039769344\n",
      "train loss:0.05724383659080546\n",
      "train loss:0.10405967248527291\n",
      "train loss:0.09761739808677827\n",
      "train loss:0.06250014599210153\n",
      "train loss:0.06810106673980487\n",
      "train loss:0.052875637166037964\n",
      "train loss:0.11651466401937081\n",
      "train loss:0.03449690378360238\n",
      "train loss:0.0394293511775107\n",
      "train loss:0.05988930907604482\n",
      "train loss:0.04813811858214603\n",
      "train loss:0.14200659765781684\n",
      "train loss:0.11547791086684336\n",
      "train loss:0.08853094740045833\n",
      "train loss:0.08534941437856512\n",
      "train loss:0.03687666059249001\n",
      "train loss:0.07247075788540258\n",
      "train loss:0.10003326675473279\n",
      "train loss:0.07230436547494903\n",
      "train loss:0.11068069720592989\n",
      "train loss:0.15299344451682576\n",
      "train loss:0.07703905437039703\n",
      "train loss:0.07655787798509107\n",
      "train loss:0.1045244664575295\n",
      "train loss:0.051384859600871795\n",
      "train loss:0.04657311162948303\n",
      "train loss:0.07540971821497826\n",
      "train loss:0.05010627175911182\n",
      "train loss:0.054288069045586876\n",
      "train loss:0.046435812070998575\n",
      "train loss:0.07243946272382376\n",
      "train loss:0.08127439504211598\n",
      "train loss:0.09735506753464113\n",
      "train loss:0.08020844628204565\n",
      "train loss:0.04393414853598616\n",
      "train loss:0.028021426233293515\n",
      "train loss:0.06861186030634775\n",
      "train loss:0.09664303226342583\n",
      "train loss:0.03296092186137999\n",
      "train loss:0.07714671876369839\n",
      "train loss:0.08782058479293825\n",
      "train loss:0.07432963081597098\n",
      "train loss:0.022039264831338\n",
      "=== epoch:11, train acc:0.969, test acc:0.955 ===\n",
      "train loss:0.070871542114946\n",
      "train loss:0.050581927259194616\n",
      "train loss:0.07881780518499304\n",
      "train loss:0.059602373802002846\n",
      "train loss:0.0948575904600291\n",
      "train loss:0.09246452876642856\n",
      "train loss:0.03655318767740543\n",
      "train loss:0.0702918986032649\n",
      "train loss:0.053105162897908446\n",
      "train loss:0.030463188759327218\n",
      "train loss:0.07501521335929526\n",
      "train loss:0.040796698359856685\n",
      "train loss:0.057419684212490905\n",
      "train loss:0.022353735861517184\n",
      "train loss:0.059082285731790155\n",
      "train loss:0.029424145054419634\n",
      "train loss:0.0660524984470028\n",
      "train loss:0.02973447354831769\n",
      "train loss:0.1242705745774372\n",
      "train loss:0.06941252055817665\n",
      "train loss:0.04525988055209891\n",
      "train loss:0.03839007606972248\n",
      "train loss:0.11137247831371828\n",
      "train loss:0.17970668043431878\n",
      "train loss:0.058099253759199956\n",
      "train loss:0.09167984699327074\n",
      "train loss:0.05978648106127226\n",
      "train loss:0.13545255452005528\n",
      "train loss:0.056164959014187804\n",
      "train loss:0.09400025175522911\n",
      "train loss:0.026328898113881866\n",
      "train loss:0.037023174154880316\n",
      "train loss:0.051907194997639135\n",
      "train loss:0.10287284500181693\n",
      "train loss:0.03261748532471557\n",
      "train loss:0.1090813304914656\n",
      "train loss:0.08095041632037402\n",
      "train loss:0.048106742046349404\n",
      "train loss:0.0972299450836101\n",
      "train loss:0.14101266009723923\n",
      "train loss:0.09865628802075936\n",
      "train loss:0.03127674700263363\n",
      "train loss:0.05901173864518159\n",
      "train loss:0.028887642915604056\n",
      "train loss:0.04753751590686222\n",
      "train loss:0.07334478012499579\n",
      "train loss:0.06211862497518845\n",
      "train loss:0.03232114311063863\n",
      "train loss:0.04593980029826117\n",
      "train loss:0.0625700564183511\n",
      "=== epoch:12, train acc:0.981, test acc:0.961 ===\n",
      "train loss:0.08603521072124538\n",
      "train loss:0.04132215986356874\n",
      "train loss:0.023376017052496544\n",
      "train loss:0.06123396657430662\n",
      "train loss:0.04399576439893047\n",
      "train loss:0.03538989464131523\n",
      "train loss:0.075213218622114\n",
      "train loss:0.07977742203850437\n",
      "train loss:0.07502638841516275\n",
      "train loss:0.05685881587000713\n",
      "train loss:0.05972521317606656\n",
      "train loss:0.014519702933704203\n",
      "train loss:0.031889208012435596\n",
      "train loss:0.027920765107370432\n",
      "train loss:0.042060959551765274\n",
      "train loss:0.09300904522740089\n",
      "train loss:0.03806595274128326\n",
      "train loss:0.0685974225510255\n",
      "train loss:0.03848152659106687\n",
      "train loss:0.08436152985160823\n",
      "train loss:0.08153609978616204\n",
      "train loss:0.030154719600049198\n",
      "train loss:0.08233190727484806\n",
      "train loss:0.027846064225686962\n",
      "train loss:0.04133223352504043\n",
      "train loss:0.022356460019480436\n",
      "train loss:0.028576601114739905\n",
      "train loss:0.042275488305581976\n",
      "train loss:0.022309343684476138\n",
      "train loss:0.07978768583654898\n",
      "train loss:0.013290291460702562\n",
      "train loss:0.03577777356114444\n",
      "train loss:0.05686721080718881\n",
      "train loss:0.039715974868337084\n",
      "train loss:0.031180668667897052\n",
      "train loss:0.04046618136495412\n",
      "train loss:0.03786688941542309\n",
      "train loss:0.036696539544809004\n",
      "train loss:0.06533096435791745\n",
      "train loss:0.03345840862144605\n",
      "train loss:0.033156238454386366\n",
      "train loss:0.04791830302207809\n",
      "train loss:0.05225968434439355\n",
      "train loss:0.0698422684493623\n",
      "train loss:0.027562651531768703\n",
      "train loss:0.017861137971116367\n",
      "train loss:0.02096090283186926\n",
      "train loss:0.038096445785628055\n",
      "train loss:0.04889484826446749\n",
      "train loss:0.04431465208845285\n",
      "=== epoch:13, train acc:0.985, test acc:0.959 ===\n",
      "train loss:0.0271654158287545\n",
      "train loss:0.023603008865923557\n",
      "train loss:0.023639484499638947\n",
      "train loss:0.03758093198967374\n",
      "train loss:0.03477672410638887\n",
      "train loss:0.03186665503721273\n",
      "train loss:0.05417224556286761\n",
      "train loss:0.03896169345138542\n",
      "train loss:0.06732967646416636\n",
      "train loss:0.04531680809079993\n",
      "train loss:0.07090174436276317\n",
      "train loss:0.07348885822805076\n",
      "train loss:0.03739027934807776\n",
      "train loss:0.016422857739045768\n",
      "train loss:0.09073602061406713\n",
      "train loss:0.13167378891296674\n",
      "train loss:0.07741909097959837\n",
      "train loss:0.021801172117988064\n",
      "train loss:0.02514250986549068\n",
      "train loss:0.02258248339470902\n",
      "train loss:0.06229804716209455\n",
      "train loss:0.04852014227422666\n",
      "train loss:0.03361715740365039\n",
      "train loss:0.03669768500820421\n",
      "train loss:0.030716145047419246\n",
      "train loss:0.018550146025715072\n",
      "train loss:0.03256212379601802\n",
      "train loss:0.02008341175955472\n",
      "train loss:0.027406633705824066\n",
      "train loss:0.03053024466586707\n",
      "train loss:0.016688244589714133\n",
      "train loss:0.054614300327210014\n",
      "train loss:0.030259418254905735\n",
      "train loss:0.04551912866485946\n",
      "train loss:0.03805726918467127\n",
      "train loss:0.053136326355569\n",
      "train loss:0.0271623085344504\n",
      "train loss:0.02694950638115211\n",
      "train loss:0.047123422740955136\n",
      "train loss:0.06733467120872846\n",
      "train loss:0.03827098719809305\n",
      "train loss:0.033462610549510295\n",
      "train loss:0.028490366432014386\n",
      "train loss:0.07383570849442252\n",
      "train loss:0.02510394750847116\n",
      "train loss:0.09474231413291777\n",
      "train loss:0.028364740970851884\n",
      "train loss:0.050677261779870274\n",
      "train loss:0.0242776222038872\n",
      "train loss:0.023230062711558745\n",
      "=== epoch:14, train acc:0.983, test acc:0.957 ===\n",
      "train loss:0.04693491886250764\n",
      "train loss:0.09740777463036934\n",
      "train loss:0.04545856689058739\n",
      "train loss:0.022721479728278194\n",
      "train loss:0.026739315063498803\n",
      "train loss:0.02518536369455588\n",
      "train loss:0.027925481646054028\n",
      "train loss:0.05002017190867065\n",
      "train loss:0.0806167059401073\n",
      "train loss:0.05263420765408444\n",
      "train loss:0.038800349037386696\n",
      "train loss:0.057753084521659545\n",
      "train loss:0.024370863415140626\n",
      "train loss:0.06226545060252756\n",
      "train loss:0.033397131226300183\n",
      "train loss:0.0200632179661532\n",
      "train loss:0.0219169238717123\n",
      "train loss:0.05317049015053883\n",
      "train loss:0.028591060969303445\n",
      "train loss:0.041856266489409044\n",
      "train loss:0.12227315864735809\n",
      "train loss:0.0306148815725638\n",
      "train loss:0.018332136381019354\n",
      "train loss:0.043821017521820396\n",
      "train loss:0.03360081866512519\n",
      "train loss:0.02181750458177407\n",
      "train loss:0.07403754939622494\n",
      "train loss:0.020493102990248584\n",
      "train loss:0.016169832214813176\n",
      "train loss:0.01899407271850123\n",
      "train loss:0.03993405688179723\n",
      "train loss:0.028599368298376823\n",
      "train loss:0.038777829481813575\n",
      "train loss:0.024231873539986116\n",
      "train loss:0.03252618591044665\n",
      "train loss:0.028822960572859414\n",
      "train loss:0.06022016548638815\n",
      "train loss:0.03566311420045297\n",
      "train loss:0.06785932182156755\n",
      "train loss:0.0318295196114147\n",
      "train loss:0.01952355116417844\n",
      "train loss:0.03415397149940855\n",
      "train loss:0.02933764080469345\n",
      "train loss:0.04164983892316115\n",
      "train loss:0.013448155398083774\n",
      "train loss:0.026884316398157714\n",
      "train loss:0.04548219670777752\n",
      "train loss:0.050986110138501184\n",
      "train loss:0.03516979379525482\n",
      "train loss:0.032700762696795824\n",
      "=== epoch:15, train acc:0.987, test acc:0.957 ===\n",
      "train loss:0.023234238608745607\n",
      "train loss:0.03307314927567264\n",
      "train loss:0.028375122375515982\n",
      "train loss:0.018635773365948727\n",
      "train loss:0.03410007131332247\n",
      "train loss:0.0560173540143011\n",
      "train loss:0.014163701334338702\n",
      "train loss:0.02317499975747068\n",
      "train loss:0.05312177195643284\n",
      "train loss:0.03124870402258357\n",
      "train loss:0.029507496653319337\n",
      "train loss:0.03302551956612732\n",
      "train loss:0.0211186345943936\n",
      "train loss:0.02206456710772013\n",
      "train loss:0.10732934015578487\n",
      "train loss:0.015303983723438282\n",
      "train loss:0.022597624744952114\n",
      "train loss:0.03440000379154592\n",
      "train loss:0.013224296739679835\n",
      "train loss:0.010918585747759908\n",
      "train loss:0.01882422159166658\n",
      "train loss:0.026463482868719076\n",
      "train loss:0.03540355329713272\n",
      "train loss:0.033972638531981715\n",
      "train loss:0.02091750718337849\n",
      "train loss:0.07265478771356321\n",
      "train loss:0.03674045291064634\n",
      "train loss:0.04191895592003093\n",
      "train loss:0.0077870288224063736\n",
      "train loss:0.02251639947917892\n",
      "train loss:0.017087267823458752\n",
      "train loss:0.011720055549676695\n",
      "train loss:0.011801753715935184\n",
      "train loss:0.012744300147730188\n",
      "train loss:0.008416083748540122\n",
      "train loss:0.024579473567752908\n",
      "train loss:0.03112312344976726\n",
      "train loss:0.035741257905484165\n",
      "train loss:0.016106983961880497\n",
      "train loss:0.013890384820941239\n",
      "train loss:0.028889276745238012\n",
      "train loss:0.017655556610400257\n",
      "train loss:0.027815171656479135\n",
      "train loss:0.05161276875110487\n",
      "train loss:0.0286999649325423\n",
      "train loss:0.04080002249611102\n",
      "train loss:0.04452481456277411\n",
      "train loss:0.046112641608105395\n",
      "train loss:0.02670301445256744\n",
      "train loss:0.01877696244136398\n",
      "=== epoch:16, train acc:0.986, test acc:0.956 ===\n",
      "train loss:0.027324145356726826\n",
      "train loss:0.02549806140816056\n",
      "train loss:0.05810321080772178\n",
      "train loss:0.017770873770171017\n",
      "train loss:0.03669606486004648\n",
      "train loss:0.01889170638933775\n",
      "train loss:0.01676541900046427\n",
      "train loss:0.02100146870363876\n",
      "train loss:0.02213215343010335\n",
      "train loss:0.013186147798548162\n",
      "train loss:0.043581400887291316\n",
      "train loss:0.03194148772931141\n",
      "train loss:0.012429567240462298\n",
      "train loss:0.019116425984640865\n",
      "train loss:0.016118804885221664\n",
      "train loss:0.01864695244715904\n",
      "train loss:0.01716169829350924\n",
      "train loss:0.011045695725402252\n",
      "train loss:0.03958809840625909\n",
      "train loss:0.020171496907399132\n",
      "train loss:0.012481775232012987\n",
      "train loss:0.01814707258524929\n",
      "train loss:0.006592059352445646\n",
      "train loss:0.006325985861832517\n",
      "train loss:0.03634049425443424\n",
      "train loss:0.03708515146245512\n",
      "train loss:0.027188436312320555\n",
      "train loss:0.04417113967777517\n",
      "train loss:0.030606045756673472\n",
      "train loss:0.019365675412916457\n",
      "train loss:0.020544301760992815\n",
      "train loss:0.02192156873955082\n",
      "train loss:0.019921775431892122\n",
      "train loss:0.01694329682161654\n",
      "train loss:0.026695098417317004\n",
      "train loss:0.02168624904623346\n",
      "train loss:0.034928981775724136\n",
      "train loss:0.011633291147516574\n",
      "train loss:0.011325474060375745\n",
      "train loss:0.01714075107578461\n",
      "train loss:0.028255909864512364\n",
      "train loss:0.02161581829668717\n",
      "train loss:0.020036857764770866\n",
      "train loss:0.025193546184551295\n",
      "train loss:0.022969814868066513\n",
      "train loss:0.010167790269792556\n",
      "train loss:0.015326594448489462\n",
      "train loss:0.015688627415807586\n",
      "train loss:0.023491969434846313\n",
      "train loss:0.022151643136998755\n",
      "=== epoch:17, train acc:0.988, test acc:0.962 ===\n",
      "train loss:0.024689657481251084\n",
      "train loss:0.03464695776483348\n",
      "train loss:0.011516942603748906\n",
      "train loss:0.013965803882903917\n",
      "train loss:0.007170673645818473\n",
      "train loss:0.0076280439917860935\n",
      "train loss:0.04156379802647917\n",
      "train loss:0.010946584022705968\n",
      "train loss:0.018708263891190558\n",
      "train loss:0.014040857467997069\n",
      "train loss:0.03322913489769611\n",
      "train loss:0.07625134205465954\n",
      "train loss:0.013922701805541709\n",
      "train loss:0.03668875563074805\n",
      "train loss:0.010384874782983033\n",
      "train loss:0.014929791150230343\n",
      "train loss:0.010859804388220371\n",
      "train loss:0.005959569548640533\n",
      "train loss:0.02499309441225295\n",
      "train loss:0.03167984103315193\n",
      "train loss:0.013657734329325709\n",
      "train loss:0.00835674873264173\n",
      "train loss:0.015893956322370707\n",
      "train loss:0.0283969605695945\n",
      "train loss:0.03209098696529534\n",
      "train loss:0.014447215009236949\n",
      "train loss:0.01167345399940465\n",
      "train loss:0.011146113461060751\n",
      "train loss:0.03385595951130194\n",
      "train loss:0.03162899209069713\n",
      "train loss:0.019398171454688033\n",
      "train loss:0.016547382255801758\n",
      "train loss:0.006847367328496825\n",
      "train loss:0.008963463277337175\n",
      "train loss:0.01603930535299341\n",
      "train loss:0.009113234426030981\n",
      "train loss:0.04315384787059014\n",
      "train loss:0.007949152257609456\n",
      "train loss:0.11880228111032576\n",
      "train loss:0.03388140351994485\n",
      "train loss:0.026137832276693675\n",
      "train loss:0.008323117370800836\n",
      "train loss:0.017903549255585502\n",
      "train loss:0.05358759835443288\n",
      "train loss:0.027839106714432522\n",
      "train loss:0.024860134538764282\n",
      "train loss:0.02293594888284484\n",
      "train loss:0.04329811028244384\n",
      "train loss:0.013127141878814343\n",
      "train loss:0.009366651829329773\n",
      "=== epoch:18, train acc:0.991, test acc:0.962 ===\n",
      "train loss:0.009911235354270473\n",
      "train loss:0.06635434593339305\n",
      "train loss:0.021774033446073152\n",
      "train loss:0.018474266866458262\n",
      "train loss:0.033485444213450746\n",
      "train loss:0.013871455481684624\n",
      "train loss:0.012366651821245444\n",
      "train loss:0.024453218493950674\n",
      "train loss:0.020772325052197886\n",
      "train loss:0.016024511915778722\n",
      "train loss:0.019325954273097287\n",
      "train loss:0.015831947554773195\n",
      "train loss:0.018997124854884483\n",
      "train loss:0.012457158148930152\n",
      "train loss:0.012321071725915768\n",
      "train loss:0.02707815898207814\n",
      "train loss:0.007427643952711379\n",
      "train loss:0.02005165949001036\n",
      "train loss:0.024513605899669205\n",
      "train loss:0.018347700818018498\n",
      "train loss:0.011804676263357423\n",
      "train loss:0.03526408019503664\n",
      "train loss:0.03194774009043895\n",
      "train loss:0.00655306068819755\n",
      "train loss:0.028701899069333004\n",
      "train loss:0.013728451575460512\n",
      "train loss:0.03261725429514394\n",
      "train loss:0.014192601401091435\n",
      "train loss:0.014012330695982143\n",
      "train loss:0.012380078121784351\n",
      "train loss:0.013922946765234418\n",
      "train loss:0.008407210533058684\n",
      "train loss:0.008016837633411976\n",
      "train loss:0.014041154561120418\n",
      "train loss:0.00839022091053649\n",
      "train loss:0.011419433633944948\n",
      "train loss:0.015623489425118488\n",
      "train loss:0.024730583899988553\n",
      "train loss:0.010274961906536722\n",
      "train loss:0.0073497736732768635\n",
      "train loss:0.018804323905433965\n",
      "train loss:0.008496252238618026\n",
      "train loss:0.032940814322150486\n",
      "train loss:0.02026081600213798\n",
      "train loss:0.024794720434287404\n",
      "train loss:0.017159033128832074\n",
      "train loss:0.009427878313509788\n",
      "train loss:0.013094470879911179\n",
      "train loss:0.0218336819350389\n",
      "train loss:0.008867354925488949\n",
      "=== epoch:19, train acc:0.995, test acc:0.96 ===\n",
      "train loss:0.019191221264414592\n",
      "train loss:0.027812286665845024\n",
      "train loss:0.012884936821178502\n",
      "train loss:0.012189281020942515\n",
      "train loss:0.033978353413624554\n",
      "train loss:0.024090585210390883\n",
      "train loss:0.023677311045957897\n",
      "train loss:0.02485591419682627\n",
      "train loss:0.0128842397458622\n",
      "train loss:0.012554334774179523\n",
      "train loss:0.004544241241750007\n",
      "train loss:0.009889256889933962\n",
      "train loss:0.00534492375999478\n",
      "train loss:0.016338943925901305\n",
      "train loss:0.010384419668626743\n",
      "train loss:0.013699859979279807\n",
      "train loss:0.018156331559704683\n",
      "train loss:0.014437921996109916\n",
      "train loss:0.019998515015545445\n",
      "train loss:0.012940817971535474\n",
      "train loss:0.009771150509465264\n",
      "train loss:0.015102689224139248\n",
      "train loss:0.05023487142290012\n",
      "train loss:0.024235742179932913\n",
      "train loss:0.009736773715825762\n",
      "train loss:0.03281463167941035\n",
      "train loss:0.02724653401927445\n",
      "train loss:0.010041629538438911\n",
      "train loss:0.006349002485869782\n",
      "train loss:0.04996964105929793\n",
      "train loss:0.011068360788379245\n",
      "train loss:0.03308495522634709\n",
      "train loss:0.016661847216205422\n",
      "train loss:0.0075030166935762845\n",
      "train loss:0.014375473774881914\n",
      "train loss:0.00622581986847236\n",
      "train loss:0.005829292011935312\n",
      "train loss:0.01912010792495211\n",
      "train loss:0.023868918369464143\n",
      "train loss:0.025821571907613215\n",
      "train loss:0.011610249010525622\n",
      "train loss:0.014168838023648886\n",
      "train loss:0.015113631242342405\n",
      "train loss:0.011728183125286153\n",
      "train loss:0.014255349347782753\n",
      "train loss:0.020295649264136656\n",
      "train loss:0.008991092602915219\n",
      "train loss:0.008528306086767825\n",
      "train loss:0.016968585576397976\n",
      "train loss:0.011349777335836822\n",
      "=== epoch:20, train acc:0.995, test acc:0.966 ===\n",
      "train loss:0.007786644596373158\n",
      "train loss:0.015819968431556062\n",
      "train loss:0.0033508717447897417\n",
      "train loss:0.00579327695108502\n",
      "train loss:0.007129870889787203\n",
      "train loss:0.02252842101832262\n",
      "train loss:0.015071421536849016\n",
      "train loss:0.007585046709367314\n",
      "train loss:0.010319431301840438\n",
      "train loss:0.004307537389719391\n",
      "train loss:0.005774679727256501\n",
      "train loss:0.005101643774161613\n",
      "train loss:0.01762334323503873\n",
      "train loss:0.004386854200514445\n",
      "train loss:0.04292710948675155\n",
      "train loss:0.007332100480836248\n",
      "train loss:0.027063094155922007\n",
      "train loss:0.010861126120197385\n",
      "train loss:0.007564851849923789\n",
      "train loss:0.004339885334675409\n",
      "train loss:0.01348654526422738\n",
      "train loss:0.007452184052290558\n",
      "train loss:0.01758737074932867\n",
      "train loss:0.018082381621771827\n",
      "train loss:0.005894708111385826\n",
      "train loss:0.010597430968634126\n",
      "train loss:0.016959538074113423\n",
      "train loss:0.01353370016356632\n",
      "train loss:0.015244211590131613\n",
      "train loss:0.012445301982236624\n",
      "train loss:0.004712508190195178\n",
      "train loss:0.02033404207304596\n",
      "train loss:0.008390016149060797\n",
      "train loss:0.010741987766729372\n",
      "train loss:0.023751922739075754\n",
      "train loss:0.005186101870879309\n",
      "train loss:0.011166207200387676\n",
      "train loss:0.018899236606013965\n",
      "train loss:0.01373453612471583\n",
      "train loss:0.007003119418541407\n",
      "train loss:0.014374568843686241\n",
      "train loss:0.01957214126633527\n",
      "train loss:0.014012503147339388\n",
      "train loss:0.019488645139857652\n",
      "train loss:0.03930559416343864\n",
      "train loss:0.0234449896005903\n",
      "train loss:0.010725332691736412\n",
      "train loss:0.0049685878299764695\n",
      "train loss:0.007516098439097141\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.963\n"
     ]
    }
   ],
   "source": [
    "from data.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
